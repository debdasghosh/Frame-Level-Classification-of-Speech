{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation, Test NumPy files loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech():\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "  \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_data(self.data_path, 'dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_data(self.data_path, 'train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(self.data_path, 'test.npy'), encoding='bytes', allow_pickle=True), None)\n",
    "        return self.test_set\n",
    "\n",
    "    \n",
    "def load_data(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes', allow_pickle=True),\n",
    "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes', allow_pickle=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, librispeech, k = 15, lowest=0.1):\n",
    "        self.k = k\n",
    "        self.x_list = librispeech[0]\n",
    "        self.y_list = librispeech[1] if len(librispeech) == 2 else None\n",
    "        self.idx_map = []\n",
    "        for i, xs in enumerate(self.x_list):\n",
    "            for j in range(xs.shape[0]):\n",
    "                self.idx_map.append((i, j))\n",
    "        \n",
    "        self.win_mask = np.concatenate((np.arange(lowest, 1.0, (1 - lowest)/k),\n",
    "                            np.arange(1.0, lowest, -(1 - lowest)/k),\n",
    "                            np.array([0.1])))\n",
    "\n",
    "        # self.win_mask = np.concatenate( (np.zeros(k), np.array([1.]), np.zeros(k)), axis=None )\n",
    "        self.win_mask = np.repeat(self.win_mask, librispeech[0][0].shape[1])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.idx_map[idx]\n",
    "        context = self.x_list[i].take(range(j - self.k, j + self.k + 1), mode='clip', axis=0).flatten()\n",
    "        context *= self.win_mask\n",
    "        xi = torch.from_numpy(context).float()\n",
    "        yi = self.y_list[i][j] if self.y_list is not None else -1\n",
    "        return xi, yi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        k = 15\n",
    "        in_size = ((k * 2) + 1) * 13\n",
    "        out_size = 346\n",
    "\n",
    "        layers = []\n",
    "        size_list = [in_size, in_size, 1024,  2048, 2048, 1024, 512, out_size, out_size]\n",
    "\n",
    "        for i in range(len(size_list) - 2):\n",
    "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "\n",
    "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        print(self.net)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(k):\n",
    "    in_size = ((k * 2) + 1) * 13\n",
    "    out_size = 346\n",
    "\n",
    "    layers = []\n",
    "    size_list = [in_size, in_size, 1024,  2048, 2048, 1024, 512, out_size, out_size]\n",
    "\n",
    "    for i in range(len(size_list) - 2):\n",
    "        layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
    "        layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "        \n",
    "    layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
    "    mynet = nn.Sequential(*layers)\n",
    "    print(mynet)\n",
    "    return mynet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader, scheduler, args):\n",
    "    model.train()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} Batch: {} [{}/{} ({:.0f}%, time:{:.2f}s)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), time.time() - t0,\n",
    "                loss.data))\n",
    "            t0 = time.time()\n",
    "    #scheduler.step()\n",
    "\n",
    "def test(model, test_loader, args):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return \"{:.4f}%\".format(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 29\n",
    "        self.lr = 0.001\n",
    "        self.cuda = True\n",
    "        self.data_dir = \"./hw1p2/\"\n",
    "        self.K = 15\n",
    "        self.seed = 1001\n",
    "        self.momentum = 0.9\n",
    "        self.log_interval = 1000\n",
    "        self.weights_dir = \"./weights/\"\n",
    "        \n",
    "args = Argument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_loader = LibriSpeech(args.data_dir)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True, 'drop_last': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(librispeech_loader.train, k=args.K),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(librispeech_loader.dev, k=args.K),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=403, out_features=403, bias=True)\n",
      "  (1): BatchNorm1d(403, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      "  (4): Linear(in_features=403, out_features=1024, bias=True)\n",
      "  (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.2, inplace=False)\n",
      "  (8): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.2, inplace=False)\n",
      "  (12): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): ReLU()\n",
      "  (15): Dropout(p=0.2, inplace=False)\n",
      "  (16): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (17): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (18): ReLU()\n",
      "  (19): Dropout(p=0.2, inplace=False)\n",
      "  (20): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (22): ReLU()\n",
      "  (23): Dropout(p=0.2, inplace=False)\n",
      "  (24): Linear(in_features=512, out_features=346, bias=True)\n",
      "  (25): BatchNorm1d(346, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (26): ReLU()\n",
      "  (27): Dropout(p=0.2, inplace=False)\n",
      "  (28): Linear(in_features=346, out_features=346, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = MyModel() # I did not ran my model this way, will give error if you try to load my pre-compiled weights(*.pth) file\n",
    "model = get_model(args.K) # Preferred way to load my model if you want to load my pre-compiled weights(*.pth) file\n",
    "model.apply(init_xavier)\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(datetime.now())\n",
    "    print('LR: ', scheduler.get_last_lr())\n",
    "    train(epoch, model, optimizer, train_loader, scheduler, args)\n",
    "    acc_str = test(model, test_loader, args)\n",
    "    if not os.path.exists(args.weights_dir):\n",
    "        os.makedirs(args.weights_dir)\n",
    "    torch.save(model.state_dict(), \"{}/hw1p2_{:03d}.pth\".format(args.weights_dir, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best weight\n",
    "#### ** Please note that you need to load model first (Model Initialization) before executing the below steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(args.weights_dir+'/hw1p2_029.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.cuda()\n",
    "            outputs = model(data)\n",
    "            predicted = outputs.data.max(1, keepdim=True)[1]\n",
    "            pred.append(predicted.cpu().numpy()[0])\n",
    "\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_loader = LibriSpeech(args.data_dir)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(librispeech_loader.test, k=args.K),\n",
    "    batch_size=1, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = eval_model(model, eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Prediction for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as w:\n",
    "    w.write('id,label\\n')\n",
    "    for i in range(len(pred)):\n",
    "            w.write(str(i)+','+str(pred[i][0])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
